{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning Minimizes Cost with Delayed Feedback in Supply Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Delayed Feedback\n",
    "\n",
    "* For example, when you are first playing a game of tic tac toe, the value of your first move is not known until you observe your opponents move\n",
    "* In this scenario the feedback from your move is **delayed**\n",
    "* The action you execute into the environment isn't directly paired with feedback for your agent to learn from\n",
    "* As seen below, one can **step back in time** & **update** the value of the previous states based on the state that the previous states led you to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/td_tictactoe.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/td_tictactoe.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Temporal Difference (TD) Learning **\n",
    "* An unsupervised learning method for prediction problems\n",
    "    * **unsupervised learning** allows for the use of intermediate information in addition to the final state in order to form a prediction\n",
    "    * whereas **supervised learning** only uses the actual outcome in order to train an agent to form a prediction\n",
    "* TD Learning takes the original state-action pair and adds the value of the difference between the feedback of the state-action pair after taking an action and the original state-action as well as adding the discounted value of the new state-action pair and updates the value of the original state-action based on the learning rate\n",
    "    * the value of the original state-action **increases** if the value of the new state-action is **greater**, and\n",
    "    * the value of the original state-action **decreases** if the value of the new state-action is **less**\n",
    "* Similar to the function of a learning rate, the value of the new state-action pair is discounted with $\\gamma$ known as the **discounting rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TD-Learning**: \n",
    "$$Q(s_t, a_t) \\leftarrow  Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma  Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TD Learning can be simiplified by staying within the same state, and allowing the agent to execute different actions and observe the outcome for the actions for the same exact state\n",
    "* By keeping the states the same, the model can be simplified to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(a_t) \\leftarrow  Q(a_t) + \\alpha (r_{t+1} + \\gamma  Q(a_{t+1}) - Q(a_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similarly, Q-Learning updates the value of a given action by adding the difference between the fedback and the given action based on the learning rate\n",
    "* The differences between TD-Leearning and Q-Learning are:\n",
    "    * The feedback is of the original action and not of the new action\n",
    "    * The calculation of the difference between the feedback and the value of the original action does not include the addition the value of the new action\n",
    "    * A $\\gamma$ parameter is not included which discounts the value of the new state-action pair\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning**: \n",
    "$$Q(a_i) \\leftarrow  Q(a_i) + \\alpha (r_t - Q(a_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these similarities and differences between TD-Learning and Q-Learning in order to modify the Q-Learning model to execute a learning process that relies on delayed feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Beer Game**\n",
    "\n",
    "* **“The Beer Game”** is a dynamic system scenario, focusing on a supply chain with five different levels from consumer to factory. The goal is to match the demand of one’s buyers exactly, minimizing both unsold products and backorders. \n",
    "\n",
    "* **Delays in the system** prevent ideal behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/beer-game-large.png\" width=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/beer-game-large.png', width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Our Game**\n",
    "\n",
    "* A supply chain comprising a:\n",
    "    - Consumer (C), Store (S), Warehouse (W), and Factory (F) \n",
    "\n",
    "* For the Warehouse, a case of beer costs: \n",
    "    - \\$0.50 to store, and \\$1 when backordered \n",
    "\n",
    "* Orders from the Factory must be placed:\n",
    "     - 1 week ahead of time, as they take 1 week to deliver. Therefore, the Warehouse must learn to predict the Store's needs 1 week beforehand\n",
    "\n",
    "**Q-Agent Warehouse** learns the optimal number of cases of beer to order to supply for the Store, based on cost minimization\n",
    "* The agent uses a Softmax Policy in order to pick actions from the state - action space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: The Credit Assignment Problem  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we assign credit to an action when many decisions contribute to the action?\n",
    "For instance, the backpropagation of error algorithm defines hidden unit error as the total weighted error signal coming from output units through connections between output units and a hidden unit.\n",
    "\n",
    "Errors are backpropagated, through adjusting the values of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: From Q to TD - Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import ADMCode\n",
    "from ADMCode import visualize as vis\n",
    "from ADMCode import believer_skeptic\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import sample as rs\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Temporary for now until push changes to PIP \n",
    "#sys.path.insert(0,'../ADMCode')\n",
    "#import believer_skeptic\n",
    "\n",
    "warnings.simplefilter('ignore', np.RankWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style='white', font_scale=1.3)\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypothesis:**\n",
    "** An agent will learn to maintain a net inventory at 0 in fewer time-steps when: **\n",
    "* α is higher\n",
    "* β is lower\n",
    "* γ is lower\n",
    "\n",
    "High learning rates may be optimal because the value of certain actions will change depending on the environment. By using a liberal strategy (low β), the agent may be more likely to explore the option of ordering lower stock, even though it may seem counterintuitive towards the goal reducing the cost of backorders at a particular time-step. Lower discount rate will help the agent integrate the feedback despite the effect of time delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing:**\n",
    "The hypotheses will be tested by choosing low, medium, and high, values of α, β, and γ for the Warehouse in each trial, which will contain twenty rounds of the Beer Game. Other suppliers will be programmed to meet consumer demand for each week, which will be held constant. The revenue from selling each unit as well as the fees for backorders and excess units will also be held constant.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
