{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning Minimizes Cost with Delayed Feedback in Supply Chain Paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Delayed Feedback: An example\n",
    "\n",
    "* When playing a game of tic-tac-toe, the value of the first move is not known until after many moves later\n",
    "* In this scenario, the **feedback for the move is delayed** because the action executed cannot be directly paired with the feedback recieved\n",
    "* When Temporal Difference Learning is applied, the agent finds the value of actions by **updating the reward values of previous states** with each move made. Through this, the agent learns to select actions that lead to the greatest total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/td_tictactoe.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/td_tictactoe.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Temporal Difference (TD) Learning **\n",
    "* ** A model-free Reinforcement Learning algorithm ** concerned with finding the optimal policy to solve a problem\n",
    "    * **Model-based** RL requires complete knowledge of the environment, including rewards and their probabilities, to find the optimal policy (ex. dynamic programming)\n",
    "    * **Model-free** RL estimates rewards and their probabilites (value funtions) by interacting with the environment to determine the optimal policy (ex. Monte Carlo methods and TD)\n",
    "    \n",
    "* A way to **estimate value functions for a particular policy**\n",
    "    * **Value functions** estimate how good a particular action will be in a given state, and can be written as:\n",
    "        * **V(s)**, the value of state, *s*, under a policy, *p*\n",
    "        * **Q(s,a)**, thae value of action, *a*, in state, *s*, under policy, *p* (aka Q-value)\n",
    "        \n",
    "* Value functions are updated at each time step\n",
    "    * TD uses **\"bootstrapping\" method**, which allows an estimate of the final reward to be calculated at each state. The state-action value is updated at every step\n",
    "    * This is unlike Monte Carlo methods, where the path taken to reach the final state is traced back and each value is updated only when the final reward is received \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-Learning Equation: \n",
    "$$Q(s_t, a_t) \\leftarrow  Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma  Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TD-Learning takes the original state-action pair and adds a learning rate modified term of the difference between the discounted value of the new state-action pair and the original state-action pair, and the new reward prediction \n",
    "     * the value of the original state-action **increases** if the value of the new state-action is **greater**, and\n",
    "     * the value of the original state-action **decreases** if the value of the new state-action is **less**\n",
    "     \n",
    "* The **discount factor**, $\\gamma$, serves as a way to weight estimates based on their placement in time   \n",
    "     * 0 < $\\gamma$ < 1, where $\\gamma$ values closer **1** place **greater value on future rewards** \n",
    "     * values closer to **0** place **greater value on immediate rewards**\n",
    "* Softmax action selection policy:\n",
    "    * Actions are weighted according to their action-value estimate\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By staying maintaing the same state, the model can be simplified to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(a_t) \\leftarrow  Q(a_t) + \\alpha (r_{t+1} + \\gamma  Q(a_{t+1}) - Q(a_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Compared to Q-Learning:\n",
    "\n",
    "* TD-Learning as On or Off Policy\n",
    "    * **On-Policy** learning requires value functions to be updated through experience\n",
    "        * Estimates the total future reward for state-action pairs assuming the current policy continues to be followed\n",
    "        * **ex. State–action–reward–state–action (SARSA)**\n",
    "    * **Off-Policy** methods update value functions using hypothetical actions, those which have not actually been tried\n",
    "        * Estimates the total future reward for state-action pairs assuming a greedy policy was followed even if it is not following a greedy policy\n",
    "        * **ex. Q-Learning**\n",
    "        \n",
    "#### Q-Learning Equation\n",
    "$$Q(a) \\leftarrow  Q(a) + \\alpha (r + \\gamma  max_\\alpha Q(a_{t+1})- Q(a))$$\n",
    "\n",
    "where ** $max_\\alpha$ is the maximum reward that attainable** in the new state and ** $\\gamma$ is set below 0** so the algorithm converges to the action-value function for an arbitrary target policy\n",
    "\n",
    "more simply: \n",
    "\n",
    "$$Q(a_i) \\leftarrow  Q(a_i) + \\alpha (r_t - Q(a_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As TD-Learning is a more general form of Q-Learning, we can broaden the above Q-Learning model to execute a **learning process employing delayed feedback by creating a SARSA model.**\n",
    "* This will allow us to use not necessarily the maximum reward to update Q-values, but to **select a new action (and therefore new reward) under the same policy as the original action**\n",
    "* Because this form takes the action selection method into account when learning, it may not arrive at the most optimal policy (learns near optimal policy), but could end up **greater overall reward** because it **updates value functions only through experience** and not based on what is assumed to be an optimal policy  \n",
    "    * A more conservative approach\n",
    "\n",
    "$$Q(a_t) \\leftarrow  Q(a_t) + \\alpha (r_{t+1} + \\gamma  Q(a_{t+1}) - Q(a_t))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link (to be deleted later) for all of the new stuff (SARSA):\n",
    "    https://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Beer Game**\n",
    "\n",
    "* **“The Beer Game”** is a dynamic system scenario, focusing on a supply chain with five different levels from consumer to factory. The goal is to match the demand of buyer exactly, so that unsold products and backorders are minimized and difference between supply and demand is equal to 0. \n",
    "\n",
    "* **Delays in the system** prevent ideal behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/beer-game-large.png\" width=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/beer-game-large.png', width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Our Game**\n",
    "\n",
    "* A supply chain comprising of:\n",
    "    - Consumer (C), Store (S), Warehouse (W), and Factory (F) \n",
    "\n",
    "* For the Warehouse, a case of beer costs: \n",
    "    - \\$0.50 to store, and \\$1 when backordered \n",
    "\n",
    "* Orders from the Factory must be placed:\n",
    "     - 1 week ahead of time, as they take 1 week to deliver. Therefore, the Warehouse must learn to predict the Store's needs 1 week beforehand\n",
    "\n",
    "**Q-Agent Warehouse** learns the optimal number of cases of beer to order to supply for the Store, based on cost minimization\n",
    "* The agent uses a Softmax Policy in order to pick actions from the state - action space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/SupplyChainz.png\" width=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/SupplyChainz.png', width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: The Credit Assignment Problem  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we assign credit to an action when many decisions contribute to the action?\n",
    "FIX\n",
    "* In the Beer Game, \n",
    "\n",
    "\n",
    "\n",
    "For instance, the backpropagation of error algorithm defines hidden unit error as the total weighted error signal coming from output units through connections between output units and a hidden unit.\n",
    "\n",
    "Errors are backpropagated, through adjusting the values of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: From Q to TD - Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import ADMCode\n",
    "from ADMCode import visualize as vis\n",
    "from ADMCode import believer_skeptic\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import sample as rs\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Temporary for now until push changes to PIP \n",
    "#sys.path.insert(0,'../ADMCode')\n",
    "#import believer_skeptic\n",
    "\n",
    "warnings.simplefilter('ignore', np.RankWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style='white', font_scale=1.3)\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe edit hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypothesis:**\n",
    "** An agent will learn to maintain a net inventory at 0 in fewer time-steps when: **\n",
    "   * \n",
    "       * \n",
    "           * $\\alpha$ is higher\n",
    "           * $\\beta$ is lower\n",
    "           * $\\gamma$ is lower\n",
    "\n",
    "* **High learning rate** may be optimal because the value of certain actions will change depending on the environment\n",
    "* **Liberal strategy**  (low $\\beta$), the agent may be more likely to explore the option of ordering lower stock, even though it may seem counterintuitive towards the goal reducing the cost of backorders at a particular time-step\n",
    "* **Lower discount rate** will help the agent integrate the feedback despite the effect of time delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing:**\n",
    "* The hypotheses will be tested by choosing **low, medium, and high**, values of $\\alpha, \\beta,$ and $\\gamma$** for the Warehouse\n",
    "    * Each **trial will contain twenty rounds** of the Beer Game\n",
    "* Other suppliers will be programmed to meet **consumer demand** for each week, which **will be held constant**. The revenue from selling each unit as well as the **fees** for backorders and excess units will also be **held constant.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
