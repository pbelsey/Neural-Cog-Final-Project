{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning Minimizes Cost with Delayed Feedback in Supply Chain Paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Delayed Feedback: An example\n",
    "\n",
    "* When playing a game of tic-tac-toe, the value of any given move is not known until many moves later\n",
    "* In this scenario, the **feedback for the move is delayed** because the action executed cannot be directly paired with the feedback recieved\n",
    "* When Temporal Difference Learning is applied, the agent finds the value of actions by **updating the reward values of previous states** with each move made. Through this, the agent learns to select actions that lead to the greatest total reward after multiple rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/td_tictactoe.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/td_tictactoe.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Temporal Difference (TD) Learning **\n",
    "* ** A model-free Reinforcement Learning algorithm ** concerned with finding the optimal policy to solve a problem\n",
    "    * **Model-based** RL requires complete knowledge of the environment, including rewards and their probabilities, to find the optimal policy (ex. dynamic programming)\n",
    "    * **Model-free** RL estimates rewards and their probabilites (value funtions) by interacting with the environment to determine the optimal policy \n",
    "        * ex. Monte Carlo methods and TD\n",
    "    \n",
    "* A way to **estimate value functions for a particular policy**\n",
    "    * **Value functions** estimate how good a particular action will be in a given state, and can be written as:\n",
    "        * **V(s)**, the value of state, *s*, under a policy, *p*\n",
    "        * **Q(s,a)**, thae value of action, *a*, in state, *s*, under policy, *p* \n",
    "            * Q-value\n",
    "        \n",
    "* Value functions are updated at each time step\n",
    "    * TD uses **\"bootstrapping\" method**, which allows an estimate of the final reward to be calculated at each state. The state-action value is updated at every step\n",
    "    * This is **unlike Monte Carlo methods**, where the path taken to reach the final state is traced back and each **value is updated only when the final reward is received**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-Learning Equation: \n",
    "$$Q(s_t, a_t) \\leftarrow  Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma  Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TD-Learning takes the original state-action pair and adds a learning rate modified term of the difference between the discounted value of the new state-action pair and the original state-action pair, in addition to a new reward prediction \n",
    "     * the value of the original state-action **increases** if the value of the new state-action is **greater**\n",
    "     * the value of the original state-action **decreases** if the value of the new state-action is **less**\n",
    "     \n",
    "* The **discount factor**, $\\gamma$, serves as a way to weight estimates based on their placement in time   \n",
    "     * 0 < $\\gamma$ < 1, where $\\gamma$ values closer **1** place **greater value on future rewards** \n",
    "     * values closer to **0** place **greater value on immediate rewards**\n",
    "* Softmax action selection policy:\n",
    "    * Actions are weighted according to their action-value estimate\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By maintaing the same state, the model can be simplified to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(a_t) \\leftarrow  Q(a_t) + \\alpha (r_{t+1} + \\gamma  Q(a_{t+1}) - Q(a_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Compared to Q-Learning:\n",
    "\n",
    "* TD-Learning as On or Off Policy\n",
    "    * **On-Policy** learning requires value functions to be updated through experience\n",
    "        * Methods estimate the total future reward for state-action pairs assuming the current policy is followed\n",
    "        * ex. State–action–reward–state–action **(SARSA)**\n",
    "    * **Off-Policy** methods update value functions using hypothetical actions, those which have not actually been tried\n",
    "        * Estimates the total future reward for state-action pairs assuming a greedy policy, even if a greedy policy was not used\n",
    "        * ex. **Q-Learning**\n",
    "        \n",
    "#### Q-Learning Equation\n",
    "$$Q(a) \\leftarrow  Q(a) + \\alpha (r + \\gamma  max_\\alpha Q(a_{t+1})- Q(a))$$\n",
    "\n",
    "where ** $max_\\alpha$ is the maximum reward attainable** in the new state and ** $\\gamma$ is set below 0** so the algorithm converges to the action-value function for a target policy\n",
    "\n",
    "simplifies to: \n",
    "\n",
    "$$Q(a_i) \\leftarrow  Q(a_i) + \\alpha (r_t - Q(a_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As TD-Learning is a more general form of Q-Learning, we can broaden the above Q-Learning model to execute a **learning process employing delayed feedback by creating a SARSA model.**\n",
    "* This will allow us to use not necessarily the maximum reward to update Q-values, but to **select a new action (and therefore new reward) under the same policy as the original action**\n",
    "* Because this form takes the action selection method into account when learning, it may not arrive at the most optimal policy (learns a near optimal policy)\n",
    "    * Could result in **greater overall reward** because SARSA **updates value functions only through experience** and not based on what is assumed to be an optimal policy  \n",
    "        * A more conservative approach\n",
    "\n",
    "$$Q(a_t) \\leftarrow  Q(a_t) + \\alpha (r_{t+1} + \\gamma  Q(a_{t+1}) - Q(a_t))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link (to be deleted later) for all of the new stuff (SARSA):\n",
    "    https://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Beer Game**\n",
    "\n",
    "**“The Beer Game”** is a dynamic system scenario, with a supply chain with five levels ranging from consumer to factory. The goal is to match the demand of buyer exactly, so that unsold products and backorders are minimized and difference between supply and demand is equal to 0 \n",
    "\n",
    "**Delays in the system** prevent ideal behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/beer-game-large.png\" width=\"800\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/beer-game-large.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Our Beer Game**\n",
    "\n",
    "* Consists of a supply chain containing:\n",
    "    - Consumer (C), Store (S), Warehouse (W), and Factory (F) \n",
    "\n",
    "* For the Warehouse, a case of beer costs: \n",
    "    - \\$0.50 to store, and \\$1 when backordered \n",
    "\n",
    "* Orders from the Factory must be placed:\n",
    "     - 1 week ahead of time, as they take 1 week to deliver. Therefore, the Warehouse must learn to predict the Store's needs 1 week beforehand\n",
    "\n",
    "**Q-Agent Warehouse learns the optimal number of cases of beer to order** to supply for the Store, **in order to minimize costs**\n",
    "   * costs are used to update Q-values\n",
    "    \n",
    "* The agent uses a Softmax Policy in order to pick actions from the state - action space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/SupplyChainz.png\" width=\"1000\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/SupplyChainz.png', width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: The Credit Assignment Problem  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we assign credit to an action when many decisions contribute to the action?\n",
    "\n",
    "* A **reward signal only weakly affects the temporally distant states** that preceded it \n",
    "    * Many iterative steps must be performed to propagate a reward signal so that it effects states and actions\n",
    "        * Errors are backpropagated, and through **many adjustments of the values of actions, a near optimal policy is converged on**\n",
    "* In the Beer Game, after many rounds the Warehous Q-Agent will learn the most rewarding number of cases to order so that backorders and overstock are near 0\n",
    "* SARSA method enables rewards to be maximized (through cost minimization), without incurring unnecessary costs that could result from Q-learning and assumption of greedy policy \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: From Q to TD - Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import ADMCode\n",
    "from ADMCode import visualize as vis\n",
    "from ADMCode import believer_skeptic\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import sample as rs\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from tdlearn import * \n",
    "\n",
    "# Temporary for now until push changes to PIP \n",
    "#sys.path.insert(0,'../ADMCode')\n",
    "#import believer_skeptic\n",
    "\n",
    "warnings.simplefilter('ignore', np.RankWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style='white', font_scale=1.3)\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe edit hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypothesis:**\n",
    "** An agent will learn to maintain a net inventory at 0 in fewer time-steps when: **\n",
    "   * \n",
    "       * \n",
    "           * $\\alpha$ is higher\n",
    "           * $\\beta$ is lower\n",
    "           * $\\gamma$ is lower\n",
    "\n",
    "* **High learning rate** may be optimal because the value of certain actions will change depending on the environment\n",
    "* **Liberal strategy**  (low $\\beta$), the agent may be more likely to explore the option of ordering lower stock, even though it may seem counterintuitive towards the goal reducing the cost of backorders at a particular time-step\n",
    "* **Lower discount rate** will help the agent integrate the feedback despite the effect of time delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing:**\n",
    "* The hypotheses will be tested by choosing **low, medium, and high, values of $\\alpha,  \\beta,$ and $\\gamma$** for the Warehouse Q - Agent\n",
    "    * Each **trial will contain twenty rounds** of the Beer Game\n",
    "* Other suppliers will be programmed to meet **consumer demand** for each week, which **will be held constant**. The revenue from selling each unit as well as the **fees** for backorders and excess units will also be **held constant.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'lowbound'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3627a88148d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtd1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTDagent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplay_bandits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Neural-Cog-Final-Project/tdlearn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, alpha, beta, gamma, epsilon, lowbound, highbound, inventory, demand)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minventory\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdemand\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlowbound\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhighbound\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlowbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parameters make no sense'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeerGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minventory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mQval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQval\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mQvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mQvector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mact_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mQvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'lowbound'"
     ]
    }
   ],
   "source": [
    "td1 = TDagent()\n",
    "play_bandits(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
