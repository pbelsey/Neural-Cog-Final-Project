{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference to Optimize Supply Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background:\n",
    "### **Temporal Difference Learning (TD)**\n",
    "* A method to learn how to predict\n",
    "* A more general form of Q - Learning, focused on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/td_tictactoe.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url='https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/td_tictactoe.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Q - Learning**: \n",
    "$$Q(a_i) \\leftarrow  Q(a_i) + \\alpha (r_t - Q(a_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic TD**: \n",
    "$$Q(s_t, a_t) \\leftarrow  Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma  Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$\n",
    "\n",
    "Through keeping states the same, $$Q(a_t) \\leftarrow  Q(a_t) + \\alpha (r_{t+1} + \\gamma  Q(a_{t+1}) - Q(a_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$$\\gamma$$** represents the discount rate of the future action state\n",
    "\n",
    "\n",
    "** $$Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$**  is the ecpected mdifference between the predicted state action and the current state action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/beer-game-large.png\" width=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url='https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/beer-game-large.png', width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Beer Game**\n",
    "\n",
    "* **“The Beer Game”** is a dynamic system scenario, focusing on a supply chain with five different levels from consumer to factory. The goal is to match the demand of one’s buyers exactly, minimizing both unsold products and backorders. \n",
    "\n",
    "* **Delays in the system** prevent ideal behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url='https://raw.githubusercontent.com/pbelsey/Neural-Cog-Final-Project/master/beer-game-large.png', width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Our Game**\n",
    "\n",
    "* A supply chain comprising a:\n",
    "    - Consumer (C), Store (S), Warehouse (W), and Factory (F) \n",
    "\n",
    "* For the Warehouse, a case of beer costs: \n",
    "    - \\$0.50 to store, and \\$1 when backordered \n",
    "\n",
    "* Orders from the Factory must be placed:\n",
    "     - 1 week ahead of time, as they take 1 week to deliver. Therefore, the Warehouse must learn to predict the Store's needs 1 week beforehand\n",
    "\n",
    "**Q - Agent Warehouse** learns the optimal number of cases of beer to order to supply for the Store, based on cost minimization\n",
    "* The agent uses a Softmax Policy in order to pick actions from the state - action space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: The Credit Assignment Problem  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we assign credit to an action when many decisions contribute to the action?\n",
    "For instance, the backpropagation of error algorithm defines hidden unit error as the total weighted error signal coming from output units through connections between output units and a hidden unit.\n",
    "\n",
    "Errors are backpropagated, through adjusting the values of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: From Q - Learning to TD  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import ADMCode\n",
    "from ADMCode import visualize as vis\n",
    "from ADMCode import believer_skeptic\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import sample as rs\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Temporary for now until push changes to PIP \n",
    "#sys.path.insert(0,'../ADMCode')\n",
    "#import believer_skeptic\n",
    "\n",
    "warnings.simplefilter('ignore', np.RankWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style='white', font_scale=1.3)\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypothesis:**\n",
    "** An agent will learn to maintain a net inventory at 0 in fewer time-steps when: **\n",
    "* α is higher\n",
    "* β is lower\n",
    "* γ is lower\n",
    "\n",
    "High learning rates may be optimal because the value of certain actions will change depending on the environment. By using a liberal strategy (low β), the agent may be more likely to explore the option of ordering lower stock, even though it may seem counterintuitive towards the goal reducing the cost of backorders at a particular time-step. Lower discount rate will help the agent integrate the feedback despite the effect of time delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing:**\n",
    "The hypotheses will be tested by choosing low, medium, and high, values of α, β, and γ for the Warehouse in each trial, which will contain twenty rounds of the Beer Game. Other suppliers will be programmed to meet consumer demand for each week, which will be held constant. The revenue from selling each unit as well as the fees for backorders and excess units will also be held constant.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
